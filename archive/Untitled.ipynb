{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abbb962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from scipy import sparse\n",
    "from pymystem3 import Mystem\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "m = Mystem()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "count_vectorizer = CountVectorizer()\n",
    "tf_vectorizer = TfidfVectorizer(use_idf=False)\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "b_tokenizer = BertTokenizer.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "model = BertModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru')\n",
    "\n",
    "\n",
    "def get_bert_corpus(texts):\n",
    "  vectors = []\n",
    "  for text in texts:\n",
    "    input_sentence = torch.tensor(b_tokenizer.encode(\"[CLS] \" + text)).unsqueeze(0)\n",
    "    out = model(input_sentence)\n",
    "    embeddings_of_last_layer = out[0]\n",
    "    cls_embeddings = embeddings_of_last_layer[0][0].detach().numpy()\n",
    "    vectors.append(cls_embeddings)\n",
    "  \n",
    "  return sparse.csr_matrix(vectors)\n",
    "\n",
    "def get_query_bert():\n",
    "    query = input('Введите ваш запрос: ')\n",
    "    query = preprocessing([query])\n",
    "    input_sentence = torch.tensor(b_tokenizer.encode(\"[CLS] \" + query[0])).unsqueeze(0)\n",
    "    out = model(input_sentence)\n",
    "    embeddings_of_last_layer = out[0]\n",
    "    cls_embeddings = embeddings_of_last_layer[0][0].detach().numpy()\n",
    "    return sparse.csr_matrix(cls_embeddings)\n",
    "\n",
    "def preprocessing(texts):\n",
    "    preprocessed_texts = []\n",
    "    for text in texts:\n",
    "        text_stripped = text.rstrip()\n",
    "        text_stripped = ' '.join(tokenizer.tokenize(text_stripped.lower()))\n",
    "        lemmas = m.lemmatize(text_stripped)\n",
    "        lemmas = [w for w in lemmas if not w.isdigit() and w != ' ']\n",
    "        preprocessed_texts.append(' '.join(lemmas))\n",
    "\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55fc332b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['слово \\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(['слово'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ae834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
